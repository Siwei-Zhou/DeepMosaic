configfile: "snake_conf.yaml"


import os


def get_sample_and_input_dicts():
    """
    read the input_files.txt and create dicts that
    correspond each sample_id with its bam file, vcf file,
    average depth value and sex information
    """
    f = open(config["input_files"], "r")
    sample = []
    bam_dict = {}
    vcf_dict = {}
    depth_dict = {}
    sex_dict = {}
    line_dict = {}
    for line in f:
        if line.startswith("#"):
            continue
        words = line.rstrip().split("\t")
        print(words)
        sample_id = words[0]
        bam_path = words[1]
        vcf_path = words[2]
        depth = words[3]
        sex = words[4]

        sample.append(sample_id)
        bam_dict[sample_id] = bam_path
        vcf_dict[sample_id] = vcf_path
        depth_dict[sample_id] = depth
        sex_dict[sample_id] = sex
        line_dict[sample_id] = [sample_id, bam_path, vcf_path, depth, sex]
    f.close()
    return sample, bam_dict, vcf_dict, depth_dict, sex_dict, line_dict


INPUT_HEADER = ["#sample_name", "bam", "vcf", "depth", "sex"]

FEATURES_HEADER = [
    "#sample_name",
    "sex",
    "chrom",
    "pos",
    "ref",
    "alt",
    "variant",
    "maf",
    "lower_CI",
    "upper_CI",
    "variant_type",
    "gene_id",
    "gnomad",
    "all_repeat",
    "segdup",
    "homopolymer",
    "dinucluotide",
    "depth_fraction",
    "image_filepath",
    "npy_filepath",
]
OUTPUT_HEADER = [
    "#sample_name",
    "sex",
    "chrom",
    "pos",
    "ref",
    "alt",
    "variant",
    "maf",
    "lower_CI",
    "upper_CI",
    "variant_type",
    "gene_id",
    "gnomad",
    "all_repeat",
    "segdup",
    "homopolymer",
    "dinucluotide",
    "depth_fraction",
    "score1",
    "score2",
    "score3",
    "prediction",
    "image_filepath",
]
(
    SAMPLE,
    BAM_DICT,
    VCF_DICT,
    DEPTH_DICT,
    SEX_DICT,
    LINE_DICT,
) = get_sample_and_input_dicts()
#print(SAMPLE)

OUT_DIR = config["out_dir"]
INPUT_FILES = config["input_files"]
ANNOVAR = config["annovar"]
DEEPMOSAIC = config["deepmosaic"]

# need to split up the feature file for prediction
# if the variant counts in it is too large
SPLITS = ["%02d" % x for x in range(10)]


# sample_id should only contains alphanumeric letter, underscore or dash
wildcard_constraints:
    sample="[a-zA-Z\d_-]+",


localrules:
    all,
    generate_sample_input_txt,
    split_features,
    filter_prediction_results,


rule all:
    """
    Dummy rule to get:
    final_summary.vcf that contains mosaic variants from all samples
    listed in the input_files.txt
    """
    input:
        expand(OUT_DIR + "/{sample}/output.txt", sample=SAMPLE),
        expand(OUT_DIR + "/{sample}/input.txt", sample=SAMPLE),
        expand(OUT_DIR + "/{sample}/features.txt", sample=SAMPLE),
        expand(OUT_DIR + "/{sample}/filtered_output.txt", sample=SAMPLE),
        OUT_DIR + "/final_summary.vcf",


rule generate_sample_input_txt:
    """
    For parallelization, create dir for each sample_id first.
    Then split the input_files.txt so that each sample_id dir 
    contains a sample_id-specific input file. Afterwards, run deepmosaic
    in parallel within each sample_id dir
    """
    output:
        outfile=OUT_DIR + "/{sample}/input.txt",
    params:
        sample_id=lambda wildcards: LINE_DICT[wildcards.sample][0],
    run:
        #if not os.path.exists(params.sample_id):
            #os.mkdir(params.sample_id)
        if not os.path.exists(output.outfile) or os.path.getsize(output.outfile) == 0:
            wfile = open(output.outfile, "w")
            wfile.write("\t".join(INPUT_HEADER) + "\n")
            wfile.write("\t".join(LINE_DICT[params.sample_id]) + "\n")
            wfile.close()


rule deepmosaic_draw:
    """
    draw feature information from variant sites
    for mosaic prediction
    """
    input:
        infile=OUT_DIR + "/{sample}/input.txt",
    output:
        feature=OUT_DIR + "/{sample}/features.txt",
    log:
        OUT_DIR + "/{sample}/logs/deepmosaic_draw.log",
    params:
        cluster="--ntasks=1 --time=168:00:00",
        target_dir=directory(OUT_DIR + "/{sample}"),
    benchmark:
        (
            OUT_DIR
            + "/{sample}/benchmark/deepmosaic_draw/{sample}.deepmosaic_draw.benchmark.txt"
        )
    conda:
        "DM"
    shell:
        "({DEEPMOSAIC}/deepmosaic-draw"
        " -i {input.infile}"
        " -o {params.target_dir}"
        " -a {ANNOVAR}) 2> {log}"


rule split_features:
    """
    if the variant counts in the feature file is too large,
    split the feature file in to 10 smaller files to run 
    deepmosaic predict in parallel
    """
    input:
        OUT_DIR + "/{sample}/features.txt",
    output:
        expand(OUT_DIR + "/{{sample}}/split/features/{split}", split=SPLITS),
    params:
        target_dir=OUT_DIR + "/{sample}/split/features/",
        file_header="\\t".join(FEATURES_HEADER),
    run:
        shell(
            """ split -d -l $(wc -l {input}|awk "{{print int((\$1+10-1)/10)}}") <(cat {input}|grep -v "#") {params.target_dir} """
        ),
        shell(
            """ for f in $(ls {params.target_dir});do sed -i "1 s/^/{params.file_header}\\n/" {params.target_dir}$f;done """
        )


rule deepmosaic_predict:
    """
    mosaic prediction based on the feature information of each variant site.
    reduce the batch size (default: 10) to reduce memory burden 
    if the input contains a large number of variants
    """
    input:
        infile=OUT_DIR + "/{sample}/split/features/{split}",
    output:
        outfile=OUT_DIR + "/{sample}/split/prediction/{split}.output.txt",
    log:
        OUT_DIR + "/{sample}/logs/{split}.deepmosaic_predict.log",
    params:
        cluster="--ntasks=12 --mem=64G --time=168:00:00",
    benchmark:
        (
            OUT_DIR
            + "/{sample}/benchmark/deepmosaic_predict/{split}.deepmosaic_predict.benchmark.txt"
        )
    conda:
        "DM"
    shell:
        "({DEEPMOSAIC}/deepmosaic-predict"
        " -i {input.infile}"
        " -b 5"
        " -o {output.outfile}"
	" -gb hg19) 2> {log}"


rule gather_prediction_results:
    """
    combine split prediction result files into a single file per sample_id
    """
    input:
        expand(
            OUT_DIR + "/{{sample}}/split/prediction/{split}.output.txt", split=SPLITS
        ),
    output:
        outfile=OUT_DIR + "/{sample}/output.txt",
    params:
        cluster="--time=1:00:00",
    run:
        wfile = open(output.outfile, "w")
        for file in input:
            print(file)
            with open(file, "r") as f:
                for line in f:
                    if line.startswith("#"):
                        continue
                    wfile.write(line)
        wfile.close()


rule filter_prediction_results:
    """
    filter out all variants that are predicted to be mosaic per sample_id
    """
    input:
        OUT_DIR + "/{sample}/output.txt",
    output:
        OUT_DIR + "/{sample}/filtered_output.txt",
    run:
        shell(""" cat {input}|awk '$22~"mosaic"{{print $0}}' >{output} """)


rule merge_filtered_results:
    """
    combine all mosaic variants from all sample_ids into a single file
    """
    input:
        expand(OUT_DIR + "/{sample}/filtered_output.txt", sample=SAMPLE),
    output:
        outfile=OUT_DIR + "/final_summary.vcf",
    params:
        cluster="--time=2:00:00",
    run:
        wfile = open(output.outfile, "w")
        wfile.write("\t".join(OUTPUT_HEADER) + "\n")
        for file in input:
            print(file)
            with open(file, "r") as f:
                for line in f:
                    if line.startswith("#"):
                        continue
                    wfile.write(line)
        wfile.close()
